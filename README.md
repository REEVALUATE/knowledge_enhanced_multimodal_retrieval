# Knowledge-Enhanced Multimodal Retrieval over Cultural Heritage Knowledge Graphs

This repository contains the experimental code, models, datasets, and implementation accompanying the ESWC 2026 In-Use Track submission:

**â€œKnowledge-Enhanced Multimodal Retrieval over Cultural Heritage Knowledge Graphsâ€**  

The goal of this repository is to fully disclose the algorithms, training code, evaluation pipelines, and system modules used in the paper. The deployment-specific backend implementation of the CH retrieval system (including API endpoints, servers, and authentication configuration, etc.) is not released.

---

## ğŸ”§ System Overview

Our proposed system integrates:

1. **A domain-adaptive multimodal retrieval module (CLIP)**  
2. **A Text2SPARQL module for knowledge reasoning over a Cultural Heritage Knowledge Graph**  

These two modules are combined through a weighted fusion strategy to support diverse queries.

### ğŸ“Œ System Architecture

The full system architecture is shown below, illustrating the multimodal CLIP retrieval and the LLM-based Text2SPARQL reasoning modules.  
![System Architecture](architecture.svg)

---

## ğŸ“¦ Released Assets

This repository publicly releases **all reproducible components** used in the paper:

### âœ”ï¸ **1. Dataset: Imageâ€“Text Pairs**
Used for CLIP fine-tuning and benchmarking.

Dataset includes:
- Artefact image  
- Automatically generated description text  
- Synthetic user-like query text  

ğŸ”— **https://huggingface.co/datasets/xuemduan/reevaluate-image-text-pairs**

(Contains ~43k imageâ€“descriptionâ€“query triplets used in experiments.)

---

### âœ”ï¸ **2. Fine-Tuned CLIP Model**

We release the CLIP ViT-L/14 domain-adapted model used in the retrieval system.

ğŸ”— **https://huggingface.co/xuemduan/reevaluate-clip**

This model supports both:
- **Text-to-Image (T2I)** retrieval  
- **Text-to-Text (T2T)** retrieval  

and is the backbone of the multimodal component.

---

### âœ”ï¸ **3. Source Code for All Experiments**

This repository includes the full implementation of:

- CLIP fine-tuning  
- CLIP evaluation (T2I, T2T, fused evaluation)  
- Synthetic dataset usage  
- Text2SPARQL pipeline  
- Knowledge-enhanced fusion evaluation  
- System-level evaluation scripts  

---

## ğŸ§± Repository Structure

```
src/
â”‚
â”œâ”€â”€ clip/
â”‚ â”œâ”€â”€ data/ # Dataset loader & preprocessing
â”‚ â”œâ”€â”€ models/ # CLIP wrapper, projection layers, fusion logic
â”‚ â”œâ”€â”€ training/ # Fine-tuning scripts (InfoNCE, mixed losses)
â”‚ â”œâ”€â”€ eval/ # T2I/T2T evaluation, metrics
â”‚ â”œâ”€â”€ utils/ # Checkpointing, logging, config handling
â”‚ â””â”€â”€ ... # (Auto-discovered on local filesystem)
â”‚
â”œâ”€â”€ text2sparql/
â”‚ â”œâ”€â”€ entity_linking/ # SPARQL-based entity resolution
â”‚ â”œâ”€â”€ json2sparql/ # Python re-implementation of Sparnatural AI logic
â”‚ â””â”€â”€ text2sparql_retrieval/ # KG querying utilities
â”‚
scripts/
â”‚ â”œâ”€â”€ train_clip.sh # CLIP fine-tuning script
â”‚ â”œâ”€â”€ eval_clip.sh # Batch evaluation scripts
â”‚ â”œâ”€â”€ run_text2sparql.sh # Text2SPARQL evaluation
â”‚ â”œâ”€â”€ run_fusion.sh # Combined evaluation
â”‚ â””â”€â”€ ...
â”‚
```

## Usage

See individual experiment folders in `scripts/` for specific running instructions.


### Text2SPARQL Module Notes
The Text2SPARQL component is:

- **Inspired by Sparnatural AI**  
  https://github.com/sparna-git/sparnatural-ai  
- Fully **re-implemented in Python** for compatibility with our backend  
- Uses a multi-stage pipeline (LLM â†’ JSON â†’ Entity Linking â†’ SPARQL)  
- Instruction prompts are adapted to match the SHACL configuration of our CH KG  
- Our own internal **Mistral Agent** is not released, but users may deploy their own agent using the provided prompt templates.
---

## ğŸš€ Usage Examples

### âš™ï¸ **1. Fine-tune CLIP**

```bash
bash scripts/train_clip.sh \
  --dataset ./data/reevaluate \
  --epochs 20 \
  --lr 5e-6 \
  --batch 64 \
  --model ViT-L-14
```

## ğŸ“ License

This repository is released under the MIT License.